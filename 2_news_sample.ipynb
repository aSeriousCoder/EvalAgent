{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parse news data from news.tsv\n",
    "save to processed/news.pkl\n",
    "format:\n",
    "    key: news_id\n",
    "    value: {\n",
    "        \"title\": str,\n",
    "        \"embedding\": torch.Tensor\n",
    "    }\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI, RateLimitError\n",
    "\n",
    "target_dir = 'MIND/small/valid'\n",
    "target_file = os.path.join(target_dir, 'news.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIServer:\n",
    "    def __init__(self):\n",
    "        self.openai_client = OpenAI(\n",
    "            api_key=\"<YOUR_OPENAI_API_KEY>\"\n",
    "        )\n",
    "\n",
    "    def embedding(self, sentences):\n",
    "        \"\"\"\n",
    "        @param sentences: list[str]\n",
    "        @return: np.array of shape (1536,)\n",
    "        \"\"\"\n",
    "        response = self.openai_client.embeddings.create(\n",
    "            input=sentences,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        embeddings = [data.embedding for data in response.data]\n",
    "        return np.array(embeddings)\n",
    "\n",
    "server = OpenAIServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_users = json.load(open('toy/processed/sampled_users.json'))\n",
    "all_used_news = set()\n",
    "for user in sampled_users:\n",
    "    all_used_news.update(user['history_news'])\n",
    "    for test in user['tests']:\n",
    "        all_used_news.update(test['news_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 1483)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_users), len(all_used_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:10<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "def parse_line(line):\n",
    "    first_split = line.find('\\t')\n",
    "    second_split = line.find('\\t', first_split+1)\n",
    "    third_split = line.find('\\t', second_split+1)\n",
    "    fourth_split = line.find('http')\n",
    "    news_id = line[:first_split]\n",
    "    category = line[first_split+1:second_split]\n",
    "    sub_category = line[second_split+1:third_split]\n",
    "    title = line[third_split+1:fourth_split].replace('\\t', ' ')\n",
    "    content = \"[{}-{}] {}\".format(category, sub_category, title)\n",
    "    return news_id, content\n",
    "\n",
    "news_pool = {}\n",
    "batch_size = 10000\n",
    "with open(target_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in tqdm(range(0, len(lines), batch_size)):\n",
    "        batch_lines = lines[i:i+batch_size]\n",
    "        batch_data = [parse_line(line) for line in batch_lines]\n",
    "        batch_data = [data for data in batch_data if data[0] in all_used_news]  # Referenced, need to be encoded\n",
    "        batch_ids = [data[0] for data in batch_data]\n",
    "        batch_contents = [data[1] for data in batch_data]\n",
    "        batch_embeddings = server.embedding(batch_contents)\n",
    "        for j in range(len(batch_ids)):\n",
    "            news_id, content, embedding = batch_ids[j], batch_contents[j], batch_embeddings[j]\n",
    "            news_pool[news_id] = {\n",
    "                \"title\": content,\n",
    "                \"embedding\": embedding\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'toy/processed'\n",
    "with open(os.path.join(output_dir, 'news.pkl'), 'wb') as f:\n",
    "    pickle.dump(news_pool, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EvalAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
